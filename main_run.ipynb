{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL for inventory control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for Using This Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook trains a deep reinforcement learning agent for inventory control using Hindsight Differentiable Policy Optimization (HDPO). Customize the problem setup and neural network architecture by editing configuration files before running. \n",
    "\n",
    "The notebook currently uses a \"raw\" dataset, in the sense that we do not pre-process or filter any data. For example, we take the sales data provided in the competition, and do not filter or try to replace missing or censored data. Additionally, we consider a simple NN architecture. Specifically, the DataDriven net represents a Multi-Layer Perceptron that takes one \"long\" vector and outputs an order quantity. Most likely, better results can be obtained by improving the quality of the dataset and using a more powerful neural architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Network Inputs\n",
    "\n",
    "The neural network receives several types of inputs, all defined in the config files. These inputs represent the state of the inventory system and relevant features.\n",
    "\n",
    "**Note on dimensions:** You will see a \"1\" in many tensor dimensions below. This represents the number of stores. In this exercise, we work with a single store (dimension = 1). The sizes below represent the sizes of the inputs given to the neural networks, as opposed to being the size of the tensors you need to create. For example, the complete dataset of Past Demands is of size `[batch, 1, past_demand_periods]`, while the size of the past demand state tensor is `[batch, 1, past_demand_periods]`.\n",
    "\n",
    "### Input Tensors\n",
    "\n",
    "1. **Inventory State** - `[batch, 1, 2]`\n",
    "   - For each product (and the unique store), the first coordinate specifies the units of inventory on hand, and the second the number of units arriving at the beginning of next week\n",
    "\n",
    "2. **Past Demands** - `[batch, 1, past_demand_periods]`\n",
    "   - Historical demand for each product\n",
    "   - Number of periods tracked set in config\n",
    "\n",
    "3. **instocks** - `[batch, 1, past_instock_periods]`\n",
    "   - Historical stock availability (in stock or not)\n",
    "   - Same structure as past demands\n",
    "\n",
    "4. **Product Features** - `[batch, 1, number_of_product_features]`\n",
    "   - Fixed product information (store ID, product ID, etc.)\n",
    "   - Does not change over time\n",
    "\n",
    "5. **Time Features** - `[features]`\n",
    "   - Time-related information common across products\n",
    "   - Examples: month, day of week, holidays\n",
    "\n",
    "6. **Time-Product Features** - `[features, batch, 1, past_periods]`\n",
    "   - Product-specific information that changes over time\n",
    "   - Examples: promotional periods, seasonal patterns\n",
    "   - `features` = number of different metrics tracked per product\n",
    "\n",
    "**See example data preparation:** Check `vn2_data_analysis.ipynb` for examples of creating these dataframes and tensors. You can create your own version of these structures by, for example, preprocessing the sales data into a better form. If you do this, remember to update the filenames in the setting config file!\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration Files\n",
    "\n",
    "You need to edit **two config files**:\n",
    "\n",
    "1. **Setting Config** (inventory problem setup): `config_files/settings/your_setting.yaml`\n",
    "2. **Policy Config** (neural network architecture): `config_files/policies_and_hyperparams/your_policy.yaml`\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters to Customize\n",
    "\n",
    "### Setting Config (`config_files/settings/`)\n",
    "\n",
    "#### Time Periods\n",
    "**Location:** `sample_data_params` and `problem_params`\n",
    "- Set `periods` for train, dev, and test datasets in `sample_data_params`\n",
    "- **Important:** Update `periods` in `problem_params` to match the ranges specified in `sample_data_params`\n",
    "\n",
    "#### Neural Network Inputs\n",
    "\n",
    "**Past demand/instock history:**\n",
    "- `observation_params/demand/past_periods` - past demand periods to include\n",
    "- `observation_params/instock/past_periods` - past instock periods to include\n",
    "\n",
    "**Time features:**\n",
    "- `observation_params/time_features_file` - filename with time data (e.g., holidays)\n",
    "- `observation_params/time_features` - column names to read and pass to neural network\n",
    "\n",
    "**Product features:**\n",
    "- `observation_params/product_features/features` - which product feature columns to use\n",
    "- `store_params/product_features/features` - **must match the above list**\n",
    "\n",
    "**Time-Product features:**\n",
    "- `store_params/time_product_features_tensor/file_location` - filename with the tensor (will use the entire tensor)\n",
    "\n",
    "#### Data Files\n",
    "**Location:** `store_params`\n",
    "- Comment out any parameters you don't need (add `#` at the start of the line). For example, if you don't need time_product_features_tensor, comment out all the info related to it\n",
    "\n",
    "---\n",
    "\n",
    "### Policy Config (`config_files/policies_and_hyperparams/`)\n",
    "\n",
    "#### Neural Network Architecture\n",
    "**Location:** `nn_params`\n",
    "- `neurons_per_hidden_layer` - layer sizes (e.g., `[64, 64]` for two layers with 64 neurons)\n",
    "- `inner_layer_activations` - activation functions (e.g., `relu`, `tanh`)\n",
    "\n",
    "Additionally, you can create your own neural network by creating a class in neural_networks.py, adding the class to the get_architecture method of NeuralNetworkCreator and creating a config that calls your architecture. See `config_files/policies_and_hyperparams/data_driven_net.yml` for an example on how to do this\n",
    "\n",
    "#### Training\n",
    "**Location:** `trainer_params` and `optimizer_params`\n",
    "- `trainer_params/epochs` - number of training epochs\n",
    "- `optimizer_params/learning_rate` - optimizer learning rate\n",
    "- `params_by_dataset/train/batch_size` - batch size for train set (you can also change for dev and test)\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- **Product features consistency:** The `features` list in `observation_params/product_features` and `store_params/product_features` must match exactly\n",
    "- **Comment out unused parameters:** Add `#` before any line you don't need\n",
    "- **Refer to README:** For complete parameter descriptions, see the \"Populating a config file\" section in the main README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and create helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from trainer import *\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_state(model, trainer, optimizer, config_setting, config_hyperparams):\n",
    "    \"\"\"\n",
    "    Get the model state dictionary with the best weights (smallest dev_loss).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The model instance\n",
    "    trainer : Trainer\n",
    "        The trainer instance containing best performance data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer instance\n",
    "    config_setting : dict\n",
    "        Configuration dictionary for settings\n",
    "    config_hyperparams : dict\n",
    "        Configuration dictionary for hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing all model information and best weights\n",
    "    \"\"\"\n",
    "    model_state = {\n",
    "        'epoch': trainer.best_epoch,\n",
    "        'model_state_dict': trainer.best_performance_data['model_params_to_save'],\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_train_loss': trainer.best_performance_data['train_loss'],\n",
    "        'best_dev_loss': trainer.best_performance_data['dev_loss'],\n",
    "        'all_train_losses': trainer.all_train_losses,\n",
    "        'all_dev_losses': trainer.all_dev_losses,\n",
    "        'all_test_losses': trainer.all_test_losses,\n",
    "        'warehouse_upper_bound': model.warehouse_upper_bound,\n",
    "        'config_setting': config_setting,\n",
    "        'config_hyperparams': config_hyperparams,\n",
    "        'save_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return model_state\n",
    "\n",
    "\n",
    "def save_trained_model(filename, model, trainer, optimizer, config_setting, config_hyperparams):\n",
    "    \"\"\"\n",
    "    Save the trained model that achieved the smallest dev_loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename : str\n",
    "        The filename to save the model as (without extension)\n",
    "    model : torch.nn.Module\n",
    "        The model instance\n",
    "    trainer : Trainer\n",
    "        The trainer instance containing best performance data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer instance\n",
    "    config_setting : dict\n",
    "        Configuration dictionary for settings\n",
    "    config_hyperparams : dict\n",
    "        Configuration dictionary for hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The full path where the model was saved\n",
    "    \"\"\"\n",
    "    # Get the best model state\n",
    "    model_state = get_best_model_state(model, trainer, optimizer, config_setting, config_hyperparams)\n",
    "    \n",
    "    # Get today's date in YYYY_MM_DD format\n",
    "    today = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    \n",
    "    # Create the directory structure: saved_models/{today's date}/\n",
    "    save_dir = f\"saved_models/{today}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the full path\n",
    "    model_path = f\"{save_dir}/{filename}.pt\"\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model_state, model_path)\n",
    "    \n",
    "    print(f\"Model saved successfully!\")\n",
    "    print(f\"Path: {model_path}\")\n",
    "    print(f\"Best dev loss: {trainer.best_performance_data['dev_loss']:.6f}\")\n",
    "    print(f\"Best epoch: {trainer.best_epoch + 1}\")\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_from_config(model, device, config_setting_file=None, config_hyperparams_file=None, \n",
    "                                inventory_state_path=None, data_dir=None, time_period_idx=None, round_predictions=True):\n",
    "    \"\"\"\n",
    "    Make predictions using a trained model by reading configuration files and automatically\n",
    "    building the observation data structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The trained model\n",
    "    device : str\n",
    "        Device to run inference on\n",
    "    config_setting_file : str, optional\n",
    "        Path to settings config file. If None, uses the current config files from the session.\n",
    "    config_hyperparams_file : str, optional  \n",
    "        Path to hyperparams config file. If None, uses the current config files from the session.\n",
    "    inventory_state_path : str, optional\n",
    "        Path to inventory state file. If None, uses default path.\n",
    "    data_dir : str, optional\n",
    "        Directory containing data files. If None, uses default path.\n",
    "    time_period_idx : int, optional\n",
    "        Time period index to use for prediction. If None, uses the last period.\n",
    "    round_predictions : bool, optional\n",
    "        Whether to round predictions to the nearest integer. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : dict\n",
    "        Dictionary containing model predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use current config if not provided\n",
    "    if config_setting_file is None:\n",
    "        config_setting_file = 'config_files/settings/vn2_round_1.yml'\n",
    "    if config_hyperparams_file is None:\n",
    "        config_hyperparams_file = 'config_files/policies_and_hyperparams/data_driven_net.yml'\n",
    "    if inventory_state_path is None:\n",
    "        inventory_state_path = 'vn2_processed_data/all_data/inventory_state.pt'\n",
    "    if data_dir is None:\n",
    "        data_dir = 'vn2_processed_data/all_data/'\n",
    "    \n",
    "    # Load configuration files\n",
    "    print(\"Loading configuration files...\")\n",
    "    with open(config_setting_file, 'r') as file:\n",
    "        config_setting = yaml.safe_load(file)\n",
    "    \n",
    "    with open(config_hyperparams_file, 'r') as file:\n",
    "        config_hyperparams = yaml.safe_load(file)\n",
    "    \n",
    "    # Extract configuration parameters\n",
    "    setting_keys = 'seeds', 'test_seeds', 'problem_params', 'params_by_dataset', 'observation_params', 'store_params', 'warehouse_params', 'echelon_params', 'sample_data_params'\n",
    "    hyperparams_keys = 'trainer_params', 'optimizer_params', 'nn_params'\n",
    "    seeds, test_seeds, problem_params, params_by_dataset, observation_params, store_params, warehouse_params, echelon_params, sample_data_params = [\n",
    "        config_setting[key] for key in setting_keys\n",
    "    ]\n",
    "    \n",
    "    observation_params = DefaultDict(lambda: None, observation_params)\n",
    "    \n",
    "    # Load data files\n",
    "    print(\"Loading data files...\")\n",
    "    inventory_data = torch.load(inventory_state_path, map_location=device)\n",
    "    \n",
    "    # Load data files based on store_params configuration\n",
    "    data_files = {}\n",
    "    if 'demand' in store_params and 'file_location' in store_params['demand']:\n",
    "        data_files['demands'] = torch.load(data_dir + store_params['demand']['file_location'].split('/')[-1], map_location=device)\n",
    "    \n",
    "    if 'instock' in store_params and 'file_location' in store_params['instock']:\n",
    "        data_files['instocks'] = torch.load(data_dir + store_params['instock']['file_location'].split('/')[-1], map_location=device)\n",
    "    \n",
    "    # Load time-product features tensor if specified\n",
    "    if 'time_product_features_tensor' in store_params and 'file_location' in store_params['time_product_features_tensor']:\n",
    "        data_files['time_product_features'] = torch.load(\n",
    "            data_dir + store_params['time_product_features_tensor']['file_location'].split('/')[-1], \n",
    "            map_location=device\n",
    "        )\n",
    "    \n",
    "    # Load time features\n",
    "    if observation_params['time_features_file']:\n",
    "        date_features = pd.read_csv(data_dir + observation_params['time_features_file'].split('/')[-1])\n",
    "        # Create a mapping from feature names to column indices\n",
    "        feature_to_index = {}\n",
    "        for i, col in enumerate(date_features.columns):\n",
    "            if col != 'date':  # Skip the date column\n",
    "                feature_to_index[col] = i - 1  # Adjust for dropped date column\n",
    "        \n",
    "        # Create tensor with features in the order specified by config\n",
    "        if observation_params['time_features']:\n",
    "            ordered_features = []\n",
    "            for feature_name in observation_params['time_features']:\n",
    "                if feature_name in feature_to_index:\n",
    "                    col_idx = feature_to_index[feature_name]\n",
    "                    ordered_features.append(date_features.iloc[:, col_idx + 1].values)  # +1 because we skip date column\n",
    "                else:\n",
    "                    print(f\"Warning: Feature '{feature_name}' not found in date features\")\n",
    "                    ordered_features.append(np.zeros(len(date_features)))\n",
    "            \n",
    "            date_features_tensor = torch.tensor(np.column_stack(ordered_features), dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            # Fallback to original method if no specific order is specified\n",
    "            date_features_tensor = torch.tensor(date_features.drop('date', axis=1).values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Load product features\n",
    "    if 'product_features' in store_params and 'file_location' in store_params['product_features']:\n",
    "        product_features = pd.read_csv(data_dir + store_params['product_features']['file_location'].split('/')[-1])\n",
    "        product_features_tensor = torch.tensor(product_features.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Determine time period for demand/instock data\n",
    "    if time_period_idx is None:\n",
    "        # Use the last period from demand data for past demands/instocks\n",
    "        if 'demands' in data_files:\n",
    "            time_period_idx = data_files['demands'].shape[2] - 1\n",
    "        else:\n",
    "            time_period_idx = 0\n",
    "\n",
    "    # ALWAYS use the last period for time features, regardless of time_period_idx parameter\n",
    "    if 'date_features_tensor' in locals():\n",
    "        time_features_idx = len(date_features_tensor) - 1\n",
    "    else:\n",
    "        time_features_idx = time_period_idx\n",
    "    \n",
    "    # Prepare past demands\n",
    "    if 'demands' in data_files and observation_params['demand']['past_periods'] > 0:\n",
    "        past_periods = observation_params['demand']['past_periods']\n",
    "        start_idx = max(0, time_period_idx - past_periods + 1)\n",
    "        past_demands = data_files['demands'][:, :, start_idx:time_period_idx+1]\n",
    "        # Pad with zeros if we don't have enough history\n",
    "        if past_demands.shape[2] < past_periods:\n",
    "            padding = torch.zeros(past_demands.shape[0], past_demands.shape[1], past_periods - past_demands.shape[2], device=device)\n",
    "            past_demands = torch.cat([padding, past_demands], dim=2)\n",
    "    \n",
    "    # Prepare past instocks\n",
    "    if 'instocks' in data_files and 'instock' in observation_params and observation_params['instock']['past_periods'] > 0:\n",
    "        past_periods = observation_params['instock']['past_periods']\n",
    "        start_idx = max(0, time_period_idx - past_periods + 1)\n",
    "        past_instocks = data_files['instocks'][:, :, start_idx:time_period_idx+1]\n",
    "        # Pad with zeros if we don't have enough history\n",
    "        if past_instocks.shape[2] < past_periods:\n",
    "            padding = torch.zeros(past_instocks.shape[0], past_instocks.shape[1], past_periods - past_instocks.shape[2], device=device)\n",
    "            past_instocks = torch.cat([padding, past_instocks], dim=2)\n",
    "    \n",
    "    # Prepare past time-product features\n",
    "    if 'time_product_features' in data_files and 'time_product_features_tensor' in observation_params and observation_params['time_product_features_tensor']['past_periods'] > 0:\n",
    "        past_periods = observation_params['time_product_features_tensor']['past_periods']\n",
    "        start_idx = max(0, time_period_idx - past_periods + 1)\n",
    "        # Original shape: [features, batch, stores, periods]\n",
    "        # Extract the relevant time window\n",
    "        time_product_features_full = data_files['time_product_features'][:, :, :, start_idx:time_period_idx+1]\n",
    "        \n",
    "        # Pad with zeros if we don't have enough history\n",
    "        if time_product_features_full.shape[3] < past_periods:\n",
    "            n_features, n_batch, n_stores, current_periods = time_product_features_full.shape\n",
    "            padding = torch.zeros(n_features, n_batch, n_stores, past_periods - current_periods, device=device)\n",
    "            time_product_features_full = torch.cat([padding, time_product_features_full], dim=3)\n",
    "    \n",
    "    # Prepare time features for the current period (use last available period)\n",
    "    if observation_params['time_features']:\n",
    "        time_features_expanded = date_features_tensor[time_features_idx:time_features_idx+1].expand(inventory_data.shape[0], -1)\n",
    "    \n",
    "    # Prepare product features\n",
    "    if 'product_features' in store_params and 'features' in store_params['product_features']:\n",
    "        feature_indices = []\n",
    "        for feature in store_params['product_features']['features']:\n",
    "            if feature in product_features.columns:\n",
    "                feature_indices.append(product_features.columns.get_loc(feature))\n",
    "        product_features_subset = product_features_tensor[:, feature_indices]\n",
    "    \n",
    "    # Create static features from store_params\n",
    "    static_features = {}\n",
    "    if observation_params['include_static_features']:\n",
    "        for feature_name, include in observation_params['include_static_features'].items():\n",
    "            if include and feature_name in store_params:\n",
    "                if 'value' in store_params[feature_name]:\n",
    "                    value = store_params[feature_name]['value']\n",
    "                    static_features[feature_name] = torch.full(\n",
    "                        (inventory_data.shape[0], inventory_data.shape[1]), \n",
    "                        value, \n",
    "                        device=device\n",
    "                    )\n",
    "    \n",
    "    # Build observation dictionary\n",
    "    observation = {\n",
    "        'store_inventories': inventory_data,\n",
    "        'current_period': torch.tensor([time_period_idx], device=device),\n",
    "    }\n",
    "    \n",
    "    # Add past demands\n",
    "    if 'past_demands' in locals():\n",
    "        observation['past_demands'] = past_demands\n",
    "    \n",
    "    # Add past instocks\n",
    "    if 'past_instocks' in locals():\n",
    "        observation['past_instocks'] = past_instocks\n",
    "    \n",
    "    # Add time-product features (each feature separately)\n",
    "    if 'time_product_features_full' in locals():\n",
    "        # Shape is [n_features, batch, stores, periods]\n",
    "        n_features = time_product_features_full.shape[0]\n",
    "        for feature_idx in range(n_features):\n",
    "            # Extract one feature: [batch, stores, periods]\n",
    "            feature_data = time_product_features_full[feature_idx]\n",
    "            observation[f'past_time_product_feature_{feature_idx}'] = feature_data\n",
    "    \n",
    "    # Add product features\n",
    "    if 'product_features_subset' in locals():\n",
    "        observation['product_features'] = product_features_subset\n",
    "    \n",
    "    # Add static features\n",
    "    observation.update(static_features)\n",
    "    \n",
    "    # Add time features\n",
    "    if observation_params['time_features']:\n",
    "        for i, feature_name in enumerate(observation_params['time_features']):\n",
    "            if i < time_features_expanded.shape[1]:\n",
    "                observation[feature_name] = time_features_expanded[:, i:i+1]\n",
    "    \n",
    "    # Add internal data\n",
    "    internal_data = {}\n",
    "    if 'demands' in data_files:\n",
    "        internal_data['demands'] = data_files['demands']\n",
    "    if 'instocks' in data_files:\n",
    "        internal_data['instocks'] = data_files['instocks']\n",
    "    if 'time_product_features' in data_files:\n",
    "        internal_data['time_product_features_tensor'] = data_files['time_product_features']\n",
    "    \n",
    "    internal_data['period_shift'] = observation_params['demand'].get('period_shift', 0)\n",
    "    observation['internal_data'] = internal_data\n",
    "    \n",
    "    # Make prediction\n",
    "    print(\"Making prediction...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(observation)\n",
    "    \n",
    "    # Round predictions if requested\n",
    "    if round_predictions:\n",
    "        predictions['stores'] = torch.round(predictions['stores'])\n",
    "    \n",
    "    # Debug output\n",
    "    print(f\"Time period index (f(for time features): {time_features_idx}\")\n",
    "    if 'past_demands' in locals():\n",
    "        print(f\"Past demands[0:3]: {past_demands[0:3]}\")\n",
    "    if 'past_instocks' in locals():\n",
    "        print(f\"Past instocks[0:3]: {past_instocks[0:3]}\")\n",
    "    if 'time_product_features_full' in locals():\n",
    "        print(f\"Time-product feature 0 [0]: {time_product_features_full[0, 0:3]}\")\n",
    "    print(f\"Inventory state[0:3]: {inventory_data[0:3]}\")\n",
    "    if observation_params['time_features']:\n",
    "        print(f\"Time features (common to all products): {time_features_expanded[0]}\")\n",
    "    if 'product_features_subset' in locals():\n",
    "        print(f\"Product features[0:3]: {product_features_subset[0:3]}\")\n",
    "    print(f\"Sample predictions (first 3): {predictions['stores'][:3]}\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save predictions in submission format\n",
    "def save_predictions_to_submission_format(predictions, output_filename=\"predictions_submission.csv\", \n",
    "                                        reference_data_path=\"vn2_data/Week 0 - 2024-04-08 - Initial State.csv\"):\n",
    "    \"\"\"\n",
    "    Save model predictions in the submission template format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : dict\n",
    "        Dictionary containing model predictions with 'stores' key\n",
    "    output_filename : str\n",
    "        Name of the output CSV file\n",
    "    reference_data_path : str\n",
    "        Path to reference data file to get Store and Product mapping\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load reference data to get Store and Product mapping\n",
    "    print(f\"Loading reference data from {reference_data_path}...\")\n",
    "    reference_data = pd.read_csv(reference_data_path)\n",
    "    \n",
    "    # Extract Store and Product columns\n",
    "    store_product_mapping = reference_data[['Store', 'Product']].copy()\n",
    "    \n",
    "    # Get predictions tensor and convert to numpy\n",
    "    predictions_tensor = predictions['stores'].cpu().numpy()  # Convert to CPU numpy array\n",
    "    \n",
    "    # Flatten predictions to match the number of store-product combinations\n",
    "    # predictions_tensor shape is [batch_size, n_stores, n_warehouses]\n",
    "    # We need to flatten it to match the number of rows in reference data\n",
    "    if predictions_tensor.ndim == 3:\n",
    "        # If 3D, flatten the last two dimensions\n",
    "        predictions_flat = predictions_tensor.reshape(-1)\n",
    "    else:\n",
    "        # If already 2D or 1D, use as is\n",
    "        predictions_flat = predictions_tensor.flatten()\n",
    "    \n",
    "    # Ensure we have the right number of predictions\n",
    "    if len(predictions_flat) != len(store_product_mapping):\n",
    "        print(f\"Warning: Number of predictions ({len(predictions_flat)}) doesn't match number of store-product combinations ({len(store_product_mapping)})\")\n",
    "        # Truncate or pad as needed\n",
    "        if len(predictions_flat) > len(store_product_mapping):\n",
    "            predictions_flat = predictions_flat[:len(store_product_mapping)]\n",
    "        else:\n",
    "            # Pad with zeros if we have fewer predictions\n",
    "            padding = np.zeros(len(store_product_mapping) - len(predictions_flat))\n",
    "            predictions_flat = np.concatenate([predictions_flat, padding])\n",
    "    \n",
    "    # Add predictions to the dataframe with column name '0'\n",
    "    store_product_mapping['0'] = predictions_flat.astype(int)\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"Saving predictions to {output_filename}...\")\n",
    "    store_product_mapping.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(store_product_mapping)} predictions to {output_filename}\")\n",
    "    print(f\"Sample predictions:\")\n",
    "    print(store_product_mapping.head(10))\n",
    "    print(f\"Prediction statistics:\")\n",
    "    print(f\"  Mean: {store_product_mapping['0'].mean():.2f}\")\n",
    "    print(f\"  Min: {store_product_mapping['0'].min()}\")\n",
    "    print(f\"  Max: {store_product_mapping['0'].max()}\")\n",
    "    print(f\"  Non-zero predictions: {(store_product_mapping['0'] > 0).sum()}\")\n",
    "    \n",
    "    return store_product_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the config files\n",
    "In this example, we will apply an MLP to a setting of one store under a lost demand assumption.\n",
    "Go to the respective config files to change the hyperparameters of the neural network or the setting, or create your own configs and call them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_setting_file = 'config_files/settings/vn2_round_1.yml'\n",
    "config_hyperparams_file = 'config_files/policies_and_hyperparams/data_driven_net.yml' # Multi-layer perceptron\n",
    "# config_hyperparams_file = 'config_files/policies_and_hyperparams/mean_last_x.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we create the datasets, trainer, optimizer, model and other instances we will need to train our agent (I do not recommend editing this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ma4177/Neural_inventory_control/data_handling.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(demand)\n"
     ]
    }
   ],
   "source": [
    "with open(config_setting_file, 'r') as file:\n",
    "    config_setting = yaml.safe_load(file)\n",
    "\n",
    "with open(config_hyperparams_file, 'r') as file:\n",
    "    config_hyperparams = yaml.safe_load(file)\n",
    "\n",
    "setting_keys = 'seeds', 'test_seeds', 'problem_params', 'params_by_dataset', 'observation_params', 'store_params', 'warehouse_params', 'echelon_params', 'sample_data_params'\n",
    "hyperparams_keys = 'trainer_params', 'optimizer_params', 'nn_params'\n",
    "seeds, test_seeds, problem_params, params_by_dataset, observation_params, store_params, warehouse_params, echelon_params, sample_data_params = [\n",
    "    config_setting[key] for key in setting_keys\n",
    "    ]\n",
    "\n",
    "trainer_params, optimizer_params, nn_params = [config_hyperparams[key] for key in hyperparams_keys]\n",
    "observation_params = DefaultDict(lambda: None, observation_params)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_creator = DatasetCreator()\n",
    "\n",
    "# For realistic data, train, dev and test sets correspond to the same products, but over disjoint periods.\n",
    "# We will therefore create one scenario, and then split the data into train, dev and test sets by \n",
    "# \"copying\" all non-period related information, and then splitting the period related information\n",
    "if sample_data_params['split_by_period']:\n",
    "    \n",
    "    scenario = Scenario(\n",
    "        periods=None,  # period info for each dataset is given in sample_data_params\n",
    "        problem_params=problem_params, \n",
    "        store_params=store_params, \n",
    "        warehouse_params=warehouse_params, \n",
    "        echelon_params=echelon_params, \n",
    "        num_samples=params_by_dataset['train']['n_samples'],  # in this case, num_samples=number of products, which has to be the same across all datasets\n",
    "        observation_params=observation_params, \n",
    "        seeds=seeds\n",
    "        )\n",
    "    \n",
    "    train_dataset, dev_dataset, test_dataset = dataset_creator.create_datasets(\n",
    "        scenario, \n",
    "        split=True, \n",
    "        by_period=True, \n",
    "        periods_for_split=[sample_data_params[k] for  k in ['train_periods', 'dev_periods', 'test_periods']],)\n",
    "\n",
    "# For synthetic data, we will first create a scenario that we will divide into train and dev sets by sample index.\n",
    "# Then, we will create a separate scenario for the test set, which will be exaclty the same as the previous scenario, \n",
    "# but with different seeds to generate demand traces, and with a longer time horizon.\n",
    "# One can use this method of generating scenarios to train a model using some specific problem primitives, \n",
    "# and then test it on a different set of problem primitives, by simply creating a new scenario with the desired primitives.\n",
    "else:\n",
    "    max_periods = max(params_by_dataset['train']['periods'], params_by_dataset['dev']['periods'])\n",
    "    scenario = Scenario(\n",
    "        periods=max_periods, \n",
    "        # periods=params_by_dataset['train']['periods'], \n",
    "        problem_params=problem_params, \n",
    "        store_params=store_params, \n",
    "        warehouse_params=warehouse_params, \n",
    "        echelon_params=echelon_params, \n",
    "        num_samples=params_by_dataset['train']['n_samples'] + params_by_dataset['dev']['n_samples'], \n",
    "        observation_params=observation_params, \n",
    "        seeds=seeds\n",
    "        )\n",
    "\n",
    "    train_dataset, dev_dataset = dataset_creator.create_datasets(scenario, split=True, by_sample_indexes=True, sample_index_for_split=params_by_dataset['dev']['n_samples'])\n",
    "\n",
    "    scenario = Scenario(\n",
    "        params_by_dataset['test']['periods'], \n",
    "        problem_params, \n",
    "        store_params, \n",
    "        warehouse_params, \n",
    "        echelon_params, \n",
    "        params_by_dataset['test']['n_samples'], \n",
    "        observation_params, \n",
    "        test_seeds\n",
    "        )\n",
    "\n",
    "    test_dataset = dataset_creator.create_datasets(scenario, split=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params_by_dataset['train']['batch_size'], shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=params_by_dataset['dev']['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params_by_dataset['test']['batch_size'], shuffle=False)\n",
    "data_loaders = {'train': train_loader, 'dev': dev_loader, 'test': test_loader}\n",
    "\n",
    "neural_net_creator = NeuralNetworkCreator\n",
    "model = neural_net_creator().create_neural_network(scenario, nn_params, device=device)\n",
    "\n",
    "loss_function = PolicyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_params['learning_rate'])\n",
    "\n",
    "simulator = Simulator(device=device)\n",
    "trainer = Trainer(device=device)\n",
    "\n",
    "# We will create a folder for each day of the year, and a subfolder for each model\n",
    "# When executing with different problem primitives (i.e. instance), it might be useful to create an additional subfolder for each instance\n",
    "trainer_params['base_dir'] = 'saved_models'\n",
    "trainer_params['save_model_folders'] = [trainer.get_year_month_day(), nn_params['name']]\n",
    "\n",
    "# We will simply name the model with the current time stamp\n",
    "trainer_params['save_model_filename'] = trainer.get_time_stamp()\n",
    "\n",
    "# Load previous model if load_model is set to True in the config file\n",
    "if trainer_params['load_previous_model']:\n",
    "    print(f'Loading model from {trainer_params[\"load_model_path\"]}')\n",
    "    model, optimizer = trainer.load_model(model, optimizer, trainer_params['load_model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load a model to continue training\n",
    "load_model = False\n",
    "if load_model:\n",
    "    model_filename = \"saved_models/2025_10_06/model_32_demands.pt\"\n",
    "    model, optimizer = trainer.load_model(model, optimizer, model_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training. You can stop training whenever you want, and you will still have access to the model later on. Sometimes it can get stuck in a bad local minima, so multiple runs may be necessary (or a different learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Average per-period train loss: 2.802469176057318\n",
      "Average per-period dev loss: 3.1358658716103416\n",
      "Best per-period dev loss: 3.1358658716103416\n",
      "epoch: 2\n",
      "Average per-period train loss: 2.6335298692710785\n",
      "Average per-period dev loss: 2.9716613133375445\n",
      "Best per-period dev loss: 2.9716613133375445\n",
      "epoch: 3\n",
      "Average per-period train loss: 2.455733960090179\n",
      "Average per-period dev loss: 2.7243597155168526\n",
      "Best per-period dev loss: 2.7243597155168526\n",
      "epoch: 4\n",
      "Average per-period train loss: 2.147103785645235\n",
      "Average per-period dev loss: 2.4199016942652167\n",
      "Best per-period dev loss: 2.4199016942652167\n",
      "epoch: 5\n",
      "Average per-period train loss: 1.8725784153922849\n",
      "Average per-period dev loss: 2.2618424781730813\n",
      "Best per-period dev loss: 2.2618424781730813\n",
      "epoch: 6\n",
      "Average per-period train loss: 1.776724255748541\n",
      "Average per-period dev loss: 2.199228091300817\n",
      "Best per-period dev loss: 2.199228091300817\n",
      "epoch: 7\n",
      "Average per-period train loss: 1.686502828565772\n",
      "Average per-period dev loss: 2.0745079778233992\n",
      "Best per-period dev loss: 2.0745079778233992\n",
      "epoch: 8\n",
      "Average per-period train loss: 1.62334692380374\n",
      "Average per-period dev loss: 2.0496995978658124\n",
      "Best per-period dev loss: 2.0496995978658124\n",
      "epoch: 9\n",
      "Average per-period train loss: 1.5678295205550918\n",
      "Average per-period dev loss: 1.9765727225556107\n",
      "Best per-period dev loss: 1.9765727225556107\n",
      "epoch: 10\n",
      "Average per-period train loss: 1.514738801893826\n",
      "Average per-period dev loss: 1.9627116421513333\n",
      "Best per-period dev loss: 1.9627116421513333\n",
      "epoch: 11\n",
      "Average per-period train loss: 1.4817278597341723\n",
      "Average per-period dev loss: 1.9525583531674413\n",
      "Best per-period dev loss: 1.9525583531674413\n",
      "epoch: 12\n",
      "Average per-period train loss: 1.4527751358210979\n",
      "Average per-period dev loss: 1.9041237084329738\n",
      "Best per-period dev loss: 1.9041237084329738\n",
      "epoch: 13\n",
      "Average per-period train loss: 1.4302287248913788\n",
      "Average per-period dev loss: 1.8507820607544105\n",
      "Best per-period dev loss: 1.8507820607544105\n",
      "epoch: 14\n",
      "Average per-period train loss: 1.4107608066060957\n",
      "Average per-period dev loss: 1.8229321180007219\n",
      "Best per-period dev loss: 1.8229321180007219\n",
      "epoch: 15\n",
      "Average per-period train loss: 1.389136572327633\n",
      "Average per-period dev loss: 1.812553227789559\n",
      "Best per-period dev loss: 1.812553227789559\n",
      "epoch: 16\n",
      "Average per-period train loss: 1.3681698731325713\n",
      "Average per-period dev loss: 1.7806902269548346\n",
      "Best per-period dev loss: 1.7806902269548346\n",
      "epoch: 17\n",
      "Average per-period train loss: 1.349376002159939\n",
      "Average per-period dev loss: 1.7585405927108244\n",
      "Best per-period dev loss: 1.7585405927108244\n",
      "epoch: 18\n",
      "Average per-period train loss: 1.3331215587678182\n",
      "Average per-period dev loss: 1.7177550633375445\n",
      "Best per-period dev loss: 1.7177550633375445\n",
      "epoch: 19\n",
      "Average per-period train loss: 1.3188673246142075\n",
      "Average per-period dev loss: 1.6901081616883995\n",
      "Best per-period dev loss: 1.6901081616883995\n",
      "epoch: 20\n",
      "Average per-period train loss: 1.3110504492218156\n",
      "Average per-period dev loss: 1.677497758087804\n",
      "Best per-period dev loss: 1.677497758087804\n",
      "epoch: 21\n",
      "Average per-period train loss: 1.3025536462123481\n",
      "Average per-period dev loss: 1.6868175562875063\n",
      "Best per-period dev loss: 1.677497758087804\n",
      "epoch: 22\n",
      "Average per-period train loss: 1.2978488871008305\n",
      "Average per-period dev loss: 1.6717096766006407\n",
      "Best per-period dev loss: 1.6717096766006407\n",
      "epoch: 23\n",
      "Average per-period train loss: 1.2913397384618952\n",
      "Average per-period dev loss: 1.661917173329423\n",
      "Best per-period dev loss: 1.661917173329423\n",
      "epoch: 24\n",
      "Average per-period train loss: 1.288609674756182\n",
      "Average per-period dev loss: 1.6655911245657176\n",
      "Best per-period dev loss: 1.661917173329423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Neural_inventory_control/trainer.py:66\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mTrain a parameterized policy\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    and the metric to use for choosing the best model\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs): \u001b[38;5;66;03m# Make multiple passes through the dataset\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Do one epoch of training, including updating the model parameters\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     average_train_loss, average_train_loss_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperiods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_periods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_periods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_train_losses\u001b[38;5;241m.\u001b[39mappend(average_train_loss_to_report)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m trainer_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_dev_every_n_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Neural_inventory_control/trainer.py:173\u001b[0m, in \u001b[0;36mTrainer.do_one_epoch\u001b[0;34m(self, optimizer, data_loader, loss_function, simulator, model, periods, problem_params, observation_params, train, ignore_periods, discrete_allocation)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Backward pass (to calculate gradient) and take gradient step\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrainable:\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mmean_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# Apply gradient clipping if specified\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_clipping_norm_value\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgradient_clipping_norm_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/neural_inventory_control/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    trainer_params['epochs'], \n",
    "    loss_function, simulator, \n",
    "    model, \n",
    "    data_loaders, \n",
    "    optimizer, \n",
    "    problem_params, \n",
    "    observation_params, \n",
    "    params_by_dataset, \n",
    "    trainer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set weights to the version that achieved the smallest dev loss (i.e., we are using early stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_state = get_best_model_state(model, trainer, optimizer, config_setting, config_hyperparams)\n",
    "model.load_state_dict(best_model_state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save the model that achieved the smallest dev_loss during training\n",
    "save_model = False\n",
    "if save_model:\n",
    "    # The function will save the model that achieved the smallest dev_loss during training\n",
    "    model_filename = \"model_32_demands\"\n",
    "    saved_model_path = save_trained_model(model_filename, model, trainer, optimizer, config_setting, config_hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load a model to evaluate. Otherwise, will use the model from training.\n",
    "load_model_for_test = False\n",
    "if load_model_for_test:\n",
    "    model_filename = \"saved_models/2025_10_06/model_32_demands.pt\"\n",
    "    model, optimizer = trainer.load_model(model, optimizer, model_filename)\n",
    "    print(f\"Loaded model from {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per-period per-product cost: 1.6665793636917385\n"
     ]
    }
   ],
   "source": [
    "average_test_loss, average_test_loss_to_report = trainer.test(\n",
    "    loss_function, \n",
    "    simulator, \n",
    "    model, \n",
    "    data_loaders, \n",
    "    optimizer, \n",
    "    problem_params, \n",
    "    observation_params, \n",
    "    params_by_dataset, \n",
    "    discrete_allocation=True # we discretize the allocation to the nearest integer\n",
    "    )\n",
    "\n",
    "print(f'Average per-period per-product cost: {average_test_loss_to_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions using the model. For this, we need `config_setting_file` and `config_hyperparams_file ` to match the ones we used during training, since this info will be used to create the state input for the neural network. If it does not exactly match, it might raise an error or (worse) we will get a prediction using wrong inputs. `inventory_state_path` is a filename to the tensor of size [products, 2] defining the inventory state for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration files...\n",
      "Loading data files...\n",
      "Making prediction...\n",
      "Time period index (f(for time features): 157\n",
      "Past demands[0:3]: tensor([[[ 0.,  0.,  2.,  2.,  0.,  0.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
      "           2.,  2.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  3.,\n",
      "           1.,  1.]],\n",
      "\n",
      "        [[ 7., 10.,  5., 11.,  7., 13.,  8., 17.,  6., 11.,  8., 12.,  6.,  7.,\n",
      "           9.,  7.]]], device='cuda:0')\n",
      "Past instocks[0:3]: tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       device='cuda:0')\n",
      "Inventory state[0:3]: tensor([[[3., 3.]],\n",
      "\n",
      "        [[1., 1.]],\n",
      "\n",
      "        [[6., 6.]]], device='cuda:0')\n",
      "Time features (common to all products): tensor([112.,  15.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.], device='cuda:0')\n",
      "Product features[0:3]: tensor([[  0., 126.],\n",
      "        [  0., 182.],\n",
      "        [  1., 124.]], device='cuda:0')\n",
      "Sample predictions (first 3): tensor([[[1.]],\n",
      "\n",
      "        [[2.]],\n",
      "\n",
      "        [[9.]]], device='cuda:0')\n",
      "Loading reference data from vn2_data/Week 0 - 2024-04-08 - Initial State.csv...\n",
      "Saving predictions to predictions/submission_20251007_181100.csv...\n",
      "Saved 599 predictions to predictions/submission_20251007_181100.csv\n",
      "Sample predictions:\n",
      "   Store  Product   0\n",
      "0      0      126   1\n",
      "1      0      182   2\n",
      "2      1      124   9\n",
      "3      2      124   8\n",
      "4      2      126   2\n",
      "5      3      126   2\n",
      "6      4      124  10\n",
      "7      4      126   1\n",
      "8      5      126   4\n",
      "9      6      124   6\n",
      "Prediction statistics:\n",
      "  Mean: 2.40\n",
      "  Min: 0\n",
      "  Max: 80\n",
      "  Non-zero predictions: 541\n"
     ]
    }
   ],
   "source": [
    "# Make sure to use the correct data directory and inventory state path!!\n",
    "# we print the inputs to the neural network, so check that they are correct\n",
    "\n",
    "predictions = make_predictions_from_config(\n",
    "    model, device,\n",
    "    config_setting_file=config_setting_file,\n",
    "    config_hyperparams_file=config_hyperparams_file,\n",
    "    inventory_state_path='vn2_processed_data/new_data/inventory_state.pt',\n",
    "    data_dir='vn2_processed_data/new_data/',\n",
    "    round_predictions=True # we round the outputs to the nearest integer\n",
    ")\n",
    "\n",
    "# optional: save predictions in submission format\n",
    "save_the_predictions = True\n",
    "if save_the_predictions:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"predictions/submission_{timestamp}.csv\"\n",
    "    save_predictions_to_submission_format(\n",
    "        predictions,\n",
    "        output_filename=output_filename,\n",
    "        reference_data_path=\"vn2_data/Week 0 - 2024-04-08 - Initial State.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_inventory_control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
