{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep RL for inventory control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for Using This Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook trains a deep reinforcement learning agent for inventory control using Hindsight Differentiable Policy Optimization (HDPO). Customize the problem setup and neural network architecture by editing configuration files before running. \n",
    "\n",
    "The notebook currently uses a \"raw\" dataset, in the sense that we do not pre-process or filter any data. For example, we take the sales data provided in the competition, and do not filter or try to replace missing or censored data. Additionally, we consider a simple NN architecture. Specifically, the DataDriven net models a Multi-Layer Perceptron that takes one \"long\" vector and outputs an order quantity.\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Network Inputs\n",
    "\n",
    "The neural network receives several types of inputs, all defined in the config files. These inputs represent the state of the inventory system and relevant features.\n",
    "\n",
    "**Note on dimensions:** You will see a \"1\" in many tensor dimensions below. This represents the number of stores. In this exercise, we work with a single store (dimension = 1).\n",
    "\n",
    "### Input Tensors\n",
    "\n",
    "1. **Inventory State** - `[batch, 1, 2]`\n",
    "   - For each product (and the unique store), the first coordinate specifies the units of inventory on hand, and the second the number of units arriving at the beginning of next week\n",
    "\n",
    "2. **Past Demands** - `[batch, 1, past_demand_periods]`\n",
    "   - Historical demand for each product\n",
    "   - Number of periods tracked set in config\n",
    "\n",
    "3. **Stockouts** - `[batch, 1, past_stockout_periods]`\n",
    "   - Historical stock availability (in stock or not)\n",
    "   - Same structure as past demands\n",
    "\n",
    "4. **Product Features** - `[batch, 1, number_of_product_features]`\n",
    "   - Fixed product information (store ID, product ID, etc.)\n",
    "   - Does not change over time\n",
    "\n",
    "5. **Time Features** - `[features]`\n",
    "   - Time-related information common across products\n",
    "   - Examples: month, day of week, holidays\n",
    "\n",
    "6. **Time-Product Features** - `[features, batch, 1, past_periods]`\n",
    "   - Product-specific information that changes over time\n",
    "   - Examples: promotional periods, seasonal patterns\n",
    "   - `features` = number of different metrics tracked per product\n",
    "\n",
    "**See example data preparation:** Check `vn2_data_analysis.ipynb` for examples of creating these dataframes and tensors. You can create your own version of these structures by, for example, preprocessing the sales data into a better form. If you do this, remember to update the filenames in the setting config file!\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration Files\n",
    "\n",
    "You need to edit **two config files**:\n",
    "\n",
    "1. **Setting Config** (inventory problem setup): `config_files/settings/your_setting.yaml`\n",
    "2. **Policy Config** (neural network architecture): `config_files/policies_and_hyperparams/your_policy.yaml`\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters to Customize\n",
    "\n",
    "### Setting Config (`config_files/settings/`)\n",
    "\n",
    "#### Time Periods\n",
    "**Location:** `sample_data_params` and `problem_params`\n",
    "- Set `periods` for train, dev, and test datasets in `sample_data_params`\n",
    "- **Important:** Update `periods` in `problem_params` to match the ranges specified in `sample_data_params`\n",
    "\n",
    "#### Neural Network Inputs\n",
    "\n",
    "**Past demand/stockout history:**\n",
    "- `observation_params/demand/past_periods` - past demand periods to include\n",
    "- `observation_params/stockout/past_periods` - past stockout periods to include\n",
    "\n",
    "**Time features:**\n",
    "- `observation_params/time_features_file` - filename with time data (e.g., holidays)\n",
    "- `observation_params/time_features` - column names to read and pass to neural network\n",
    "\n",
    "**Product features:**\n",
    "- `observation_params/product_features/features` - which product feature columns to use\n",
    "- `store_params/product_features/features` - **must match the above list**\n",
    "\n",
    "**Time-Product features:**\n",
    "- `store_params/time_product_features_tensor/file_location` - filename with the tensor (will use the entire tensor)\n",
    "\n",
    "#### Data Files\n",
    "**Location:** `store_params`\n",
    "- Comment out any parameters you don't need (add `#` at the start of the line). For example, if you don't need time_product_features_tensor, comment out all the info related to it\n",
    "\n",
    "---\n",
    "\n",
    "### Policy Config (`config_files/policies_and_hyperparams/`)\n",
    "\n",
    "#### Neural Network Architecture\n",
    "**Location:** `nn_params`\n",
    "- `neurons_per_hidden_layer` - layer sizes (e.g., `[64, 64]` for two layers with 64 neurons)\n",
    "- `inner_layer_activations` - activation functions (e.g., `relu`, `tanh`)\n",
    "\n",
    "Additionally, you can create your own neural network by creating a class in neural_networks.py, adding the class to the get_architecture method of NeuralNetworkCreator and creating a config that calls your architecture. See `config_files/policies_and_hyperparams/data_driven_net.yml` for an example on how to do this\n",
    "\n",
    "#### Training\n",
    "**Location:** `trainer_params` and `optimizer_params`\n",
    "- `trainer_params/epochs` - number of training epochs\n",
    "- `optimizer_params/learning_rate` - optimizer learning rate\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- **Product features consistency:** The `features` list in `observation_params/product_features` and `store_params/product_features` must match exactly\n",
    "- **Comment out unused parameters:** Add `#` before any line you don't need\n",
    "- **Refer to README:** For complete parameter descriptions, see the \"Populating a config file\" section in the main README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and create helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "from trainer import *\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_state(model, trainer, optimizer, config_setting, config_hyperparams):\n",
    "    \"\"\"\n",
    "    Get the model state dictionary with the best weights (smallest dev_loss).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The model instance\n",
    "    trainer : Trainer\n",
    "        The trainer instance containing best performance data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer instance\n",
    "    config_setting : dict\n",
    "        Configuration dictionary for settings\n",
    "    config_hyperparams : dict\n",
    "        Configuration dictionary for hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing all model information and best weights\n",
    "    \"\"\"\n",
    "    model_state = {\n",
    "        'epoch': trainer.best_epoch,\n",
    "        'model_state_dict': trainer.best_performance_data['model_params_to_save'],\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_train_loss': trainer.best_performance_data['train_loss'],\n",
    "        'best_dev_loss': trainer.best_performance_data['dev_loss'],\n",
    "        'all_train_losses': trainer.all_train_losses,\n",
    "        'all_dev_losses': trainer.all_dev_losses,\n",
    "        'all_test_losses': trainer.all_test_losses,\n",
    "        'warehouse_upper_bound': model.warehouse_upper_bound,\n",
    "        'config_setting': config_setting,\n",
    "        'config_hyperparams': config_hyperparams,\n",
    "        'save_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return model_state\n",
    "\n",
    "\n",
    "def save_trained_model(filename, model, trainer, optimizer, config_setting, config_hyperparams):\n",
    "    \"\"\"\n",
    "    Save the trained model that achieved the smallest dev_loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename : str\n",
    "        The filename to save the model as (without extension)\n",
    "    model : torch.nn.Module\n",
    "        The model instance\n",
    "    trainer : Trainer\n",
    "        The trainer instance containing best performance data\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer instance\n",
    "    config_setting : dict\n",
    "        Configuration dictionary for settings\n",
    "    config_hyperparams : dict\n",
    "        Configuration dictionary for hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The full path where the model was saved\n",
    "    \"\"\"\n",
    "    # Get the best model state\n",
    "    model_state = get_best_model_state(model, trainer, optimizer, config_setting, config_hyperparams)\n",
    "    \n",
    "    # Get today's date in YYYY_MM_DD format\n",
    "    today = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "    \n",
    "    # Create the directory structure: saved_models/{today's date}/\n",
    "    save_dir = f\"saved_models/{today}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the full path\n",
    "    model_path = f\"{save_dir}/{filename}.pt\"\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model_state, model_path)\n",
    "    \n",
    "    print(f\"Model saved successfully!\")\n",
    "    print(f\"Path: {model_path}\")\n",
    "    print(f\"Best dev loss: {trainer.best_performance_data['dev_loss']:.6f}\")\n",
    "    print(f\"Best epoch: {trainer.best_epoch + 1}\")\n",
    "    \n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_from_config(model, device, config_setting_file=None, config_hyperparams_file=None, \n",
    "                                inventory_state_path=None, data_dir=None, time_period_idx=None, round_predictions=True):\n",
    "    \"\"\"\n",
    "    Make predictions using a trained model by reading configuration files and automatically\n",
    "    building the observation data structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The trained model\n",
    "    device : str\n",
    "        Device to run inference on\n",
    "    config_setting_file : str, optional\n",
    "        Path to settings config file. If None, uses the current config files from the session.\n",
    "    config_hyperparams_file : str, optional  \n",
    "        Path to hyperparams config file. If None, uses the current config files from the session.\n",
    "    inventory_state_path : str, optional\n",
    "        Path to inventory state file. If None, uses default path.\n",
    "    data_dir : str, optional\n",
    "        Directory containing data files. If None, uses default path.\n",
    "    time_period_idx : int, optional\n",
    "        Time period index to use for prediction. If None, uses the last period.\n",
    "    round_predictions : bool, optional\n",
    "        Whether to round predictions to the nearest integer. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : dict\n",
    "        Dictionary containing model predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use current config if not provided\n",
    "    if config_setting_file is None:\n",
    "        config_setting_file = 'config_files/settings/vn2_round_1.yml'\n",
    "    if config_hyperparams_file is None:\n",
    "        config_hyperparams_file = 'config_files/policies_and_hyperparams/data_driven_net.yml'\n",
    "    if inventory_state_path is None:\n",
    "        inventory_state_path = 'vn2_processed_data/all_data/inventory_state.pt'\n",
    "    if data_dir is None:\n",
    "        data_dir = 'vn2_processed_data/all_data/'\n",
    "    \n",
    "    # Load configuration files\n",
    "    print(\"Loading configuration files...\")\n",
    "    with open(config_setting_file, 'r') as file:\n",
    "        config_setting = yaml.safe_load(file)\n",
    "    \n",
    "    with open(config_hyperparams_file, 'r') as file:\n",
    "        config_hyperparams = yaml.safe_load(file)\n",
    "    \n",
    "    # Extract configuration parameters\n",
    "    setting_keys = 'seeds', 'test_seeds', 'problem_params', 'params_by_dataset', 'observation_params', 'store_params', 'warehouse_params', 'echelon_params', 'sample_data_params'\n",
    "    hyperparams_keys = 'trainer_params', 'optimizer_params', 'nn_params'\n",
    "    seeds, test_seeds, problem_params, params_by_dataset, observation_params, store_params, warehouse_params, echelon_params, sample_data_params = [\n",
    "        config_setting[key] for key in setting_keys\n",
    "    ]\n",
    "    \n",
    "    observation_params = DefaultDict(lambda: None, observation_params)\n",
    "    \n",
    "    # Load data files\n",
    "    print(\"Loading data files...\")\n",
    "    inventory_data = torch.load(inventory_state_path, map_location=device)\n",
    "    \n",
    "    # Load data files based on store_params configuration\n",
    "    data_files = {}\n",
    "    if 'demand' in store_params and 'file_location' in store_params['demand']:\n",
    "        data_files['demands'] = torch.load(data_dir + store_params['demand']['file_location'].split('/')[-1], map_location=device)\n",
    "    \n",
    "    if 'stockout' in store_params and 'file_location' in store_params['stockout']:\n",
    "        data_files['stockouts'] = torch.load(data_dir + store_params['stockout']['file_location'].split('/')[-1], map_location=device)\n",
    "    \n",
    "    # Load time features\n",
    "    if observation_params['time_features_file']:\n",
    "        date_features = pd.read_csv(data_dir + observation_params['time_features_file'].split('/')[-1])\n",
    "        # Create a mapping from feature names to column indices\n",
    "        feature_to_index = {}\n",
    "        for i, col in enumerate(date_features.columns):\n",
    "            if col != 'date':  # Skip the date column\n",
    "                feature_to_index[col] = i - 1  # Adjust for dropped date column\n",
    "        \n",
    "        # Create tensor with features in the order specified by config\n",
    "        if observation_params['time_features']:\n",
    "            ordered_features = []\n",
    "            for feature_name in observation_params['time_features']:\n",
    "                if feature_name in feature_to_index:\n",
    "                    col_idx = feature_to_index[feature_name]\n",
    "                    ordered_features.append(date_features.iloc[:, col_idx + 1].values)  # +1 because we skip date column\n",
    "                else:\n",
    "                    print(f\"Warning: Feature '{feature_name}' not found in date features\")\n",
    "                    ordered_features.append(np.zeros(len(date_features)))\n",
    "            \n",
    "            date_features_tensor = torch.tensor(np.column_stack(ordered_features), dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            # Fallback to original method if no specific order is specified\n",
    "            date_features_tensor = torch.tensor(date_features.drop('date', axis=1).values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Load product features\n",
    "    if 'product_features' in store_params and 'file_location' in store_params['product_features']:\n",
    "        product_features = pd.read_csv(data_dir + store_params['product_features']['file_location'].split('/')[-1])\n",
    "        product_features_tensor = torch.tensor(product_features.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Determine time period\n",
    "    if time_period_idx is None:\n",
    "        # Use the last period\n",
    "        if 'demands' in data_files:\n",
    "            time_period_idx = data_files['demands'].shape[2] - 1\n",
    "        else:\n",
    "            time_period_idx = -1\n",
    "    \n",
    "    # Prepare past demands\n",
    "    if 'demands' in data_files and observation_params['demand']['past_periods'] > 0:\n",
    "        past_periods = observation_params['demand']['past_periods']\n",
    "        start_idx = max(0, time_period_idx - past_periods + 1)\n",
    "        past_demands = data_files['demands'][:, :, start_idx:time_period_idx+1]\n",
    "        # Pad with zeros if we don't have enough history\n",
    "        if past_demands.shape[2] < past_periods:\n",
    "            padding = torch.zeros(past_demands.shape[0], past_demands.shape[1], past_periods - past_demands.shape[2], device=device)\n",
    "            past_demands = torch.cat([padding, past_demands], dim=2)\n",
    "    \n",
    "    # Prepare past stockouts\n",
    "    if 'stockouts' in data_files and 'stockout' in observation_params and observation_params['stockout']['past_periods'] > 0:\n",
    "        past_periods = observation_params['stockout']['past_periods']\n",
    "        start_idx = max(0, time_period_idx - past_periods + 1)\n",
    "        past_stockouts = data_files['stockouts'][:, :, start_idx:time_period_idx+1]\n",
    "        # Pad with zeros if we don't have enough history\n",
    "        if past_stockouts.shape[2] < past_periods:\n",
    "            padding = torch.zeros(past_stockouts.shape[0], past_stockouts.shape[1], past_periods - past_stockouts.shape[2], device=device)\n",
    "            past_stockouts = torch.cat([padding, past_stockouts], dim=2)\n",
    "    \n",
    "    # Prepare time features for the current period\n",
    "    if observation_params['time_features']:\n",
    "        time_features_expanded = date_features_tensor[time_period_idx:time_period_idx+1].expand(inventory_data.shape[0], -1)\n",
    "    \n",
    "    # Prepare product features\n",
    "    if 'product_features' in store_params and 'features' in store_params['product_features']:\n",
    "        feature_indices = []\n",
    "        for feature in store_params['product_features']['features']:\n",
    "            if feature in product_features.columns:\n",
    "                feature_indices.append(product_features.columns.get_loc(feature))\n",
    "        product_features_subset = product_features_tensor[:, feature_indices]\n",
    "    \n",
    "    # Create static features from store_params\n",
    "    static_features = {}\n",
    "    if observation_params['include_static_features']:\n",
    "        for feature_name, include in observation_params['include_static_features'].items():\n",
    "            if include and feature_name in store_params:\n",
    "                if 'value' in store_params[feature_name]:\n",
    "                    value = store_params[feature_name]['value']\n",
    "                    static_features[feature_name] = torch.full(\n",
    "                        (inventory_data.shape[0], inventory_data.shape[1]), \n",
    "                        value, \n",
    "                        device=device\n",
    "                    )\n",
    "    \n",
    "    # Build observation dictionary\n",
    "    observation = {\n",
    "        'store_inventories': inventory_data,\n",
    "        'current_period': torch.tensor([time_period_idx], device=device),\n",
    "    }\n",
    "    \n",
    "    # Add past demands\n",
    "    if 'past_demands' in locals():\n",
    "        observation['past_demands'] = past_demands\n",
    "    \n",
    "    # Add past stockouts\n",
    "    if 'past_stockouts' in locals():\n",
    "        observation['past_stockouts'] = past_stockouts\n",
    "    \n",
    "    # Add product features\n",
    "    if 'product_features_subset' in locals():\n",
    "        observation['product_features'] = product_features_subset\n",
    "    \n",
    "    # Add static features\n",
    "    observation.update(static_features)\n",
    "    \n",
    "    # Add time features\n",
    "    if observation_params['time_features']:\n",
    "        for i, feature_name in enumerate(observation_params['time_features']):\n",
    "            if i < time_features_expanded.shape[1]:\n",
    "                observation[feature_name] = time_features_expanded[:, i:i+1]\n",
    "    \n",
    "    # Add internal data\n",
    "    internal_data = {}\n",
    "    if 'demands' in data_files:\n",
    "        internal_data['demands'] = data_files['demands']\n",
    "    if 'stockouts' in data_files:\n",
    "        internal_data['stockouts'] = data_files['stockouts']\n",
    "    \n",
    "    internal_data['period_shift'] = observation_params['demand'].get('period_shift', 0)\n",
    "    observation['internal_data'] = internal_data\n",
    "    \n",
    "    # Make prediction\n",
    "    print(\"Making prediction...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(observation)\n",
    "    \n",
    "    # Round predictions if requested\n",
    "    if round_predictions:\n",
    "        predictions['stores'] = torch.round(predictions['stores'])\n",
    "    \n",
    "    # print(f\"Predictions shape: {predictions['stores'].shape}\")\n",
    "    print(f\"Past demands[0]: {past_demands[0:10]}\")\n",
    "    print(f\"Past stockouts[0]: {past_stockouts[0:10]}\")\n",
    "    print(f\"Inventory state[0]: {inventory_data[0:10]}\")\n",
    "    # print(f\"Mean of predictions: {predictions['stores'].mean()}\")\n",
    "    # print(f\"Time period used: {time_period_idx}\")\n",
    "    \n",
    "    print(f\"Time features[0]: {time_features_expanded[0]}\")\n",
    "    print(f\"Sample predictions (first 10): {predictions['stores'][:10]}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Function to save predictions in submission format\n",
    "def save_predictions_to_submission_format(predictions, output_filename=\"predictions_submission.csv\", \n",
    "                                        reference_data_path=\"vn2_data/Week 0 - 2024-04-08 - Initial State.csv\"):\n",
    "    \"\"\"\n",
    "    Save model predictions in the submission template format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : dict\n",
    "        Dictionary containing model predictions with 'stores' key\n",
    "    output_filename : str\n",
    "        Name of the output CSV file\n",
    "    reference_data_path : str\n",
    "        Path to reference data file to get Store and Product mapping\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load reference data to get Store and Product mapping\n",
    "    print(f\"Loading reference data from {reference_data_path}...\")\n",
    "    reference_data = pd.read_csv(reference_data_path)\n",
    "    \n",
    "    # Extract Store and Product columns\n",
    "    store_product_mapping = reference_data[['Store', 'Product']].copy()\n",
    "    \n",
    "    # Get predictions tensor and convert to numpy\n",
    "    predictions_tensor = predictions['stores'].cpu().numpy()  # Convert to CPU numpy array\n",
    "    \n",
    "    # Flatten predictions to match the number of store-product combinations\n",
    "    # predictions_tensor shape is [batch_size, n_stores, n_warehouses]\n",
    "    # We need to flatten it to match the number of rows in reference data\n",
    "    if predictions_tensor.ndim == 3:\n",
    "        # If 3D, flatten the last two dimensions\n",
    "        predictions_flat = predictions_tensor.reshape(-1)\n",
    "    else:\n",
    "        # If already 2D or 1D, use as is\n",
    "        predictions_flat = predictions_tensor.flatten()\n",
    "    \n",
    "    # Ensure we have the right number of predictions\n",
    "    if len(predictions_flat) != len(store_product_mapping):\n",
    "        print(f\"Warning: Number of predictions ({len(predictions_flat)}) doesn't match number of store-product combinations ({len(store_product_mapping)})\")\n",
    "        # Truncate or pad as needed\n",
    "        if len(predictions_flat) > len(store_product_mapping):\n",
    "            predictions_flat = predictions_flat[:len(store_product_mapping)]\n",
    "        else:\n",
    "            # Pad with zeros if we have fewer predictions\n",
    "            padding = np.zeros(len(store_product_mapping) - len(predictions_flat))\n",
    "            predictions_flat = np.concatenate([predictions_flat, padding])\n",
    "    \n",
    "    # Add predictions to the dataframe\n",
    "    store_product_mapping['Prediction'] = predictions_flat.astype(int)\n",
    "    \n",
    "    # Save to CSV\n",
    "    print(f\"Saving predictions to {output_filename}...\")\n",
    "    store_product_mapping.to_csv(output_filename, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(store_product_mapping)} predictions to {output_filename}\")\n",
    "    print(f\"Sample predictions:\")\n",
    "    print(store_product_mapping.head(10))\n",
    "    print(f\"Prediction statistics:\")\n",
    "    print(f\"  Mean: {store_product_mapping['Prediction'].mean():.2f}\")\n",
    "    print(f\"  Min: {store_product_mapping['Prediction'].min()}\")\n",
    "    print(f\"  Max: {store_product_mapping['Prediction'].max()}\")\n",
    "    print(f\"  Non-zero predictions: {(store_product_mapping['Prediction'] > 0).sum()}\")\n",
    "    \n",
    "    return store_product_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the config files\n",
    "In this example, we will apply an MLP to a setting of one store under a lost demand assumption.\n",
    "Go to the respective config files to change the hyperparameters of the neural network or the setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_setting_file = 'config_files/settings/vn2_round_1.yml'\n",
    "config_hyperparams_file = 'config_files/policies_and_hyperparams/data_driven_net.yml' # Multi-layer perceptron\n",
    "# config_hyperparams_file = 'config_files/policies_and_hyperparams/mean_last_x.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we create the datasets, trainer, optimizer, model and other instances we will need to train our agent (I do not recommend editing this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/ma4177/Neural_inventory_control/data_handling.py:188: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(demand)\n"
     ]
    }
   ],
   "source": [
    "with open(config_setting_file, 'r') as file:\n",
    "    config_setting = yaml.safe_load(file)\n",
    "\n",
    "with open(config_hyperparams_file, 'r') as file:\n",
    "    config_hyperparams = yaml.safe_load(file)\n",
    "\n",
    "setting_keys = 'seeds', 'test_seeds', 'problem_params', 'params_by_dataset', 'observation_params', 'store_params', 'warehouse_params', 'echelon_params', 'sample_data_params'\n",
    "hyperparams_keys = 'trainer_params', 'optimizer_params', 'nn_params'\n",
    "seeds, test_seeds, problem_params, params_by_dataset, observation_params, store_params, warehouse_params, echelon_params, sample_data_params = [\n",
    "    config_setting[key] for key in setting_keys\n",
    "    ]\n",
    "\n",
    "trainer_params, optimizer_params, nn_params = [config_hyperparams[key] for key in hyperparams_keys]\n",
    "observation_params = DefaultDict(lambda: None, observation_params)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_creator = DatasetCreator()\n",
    "\n",
    "# For realistic data, train, dev and test sets correspond to the same products, but over disjoint periods.\n",
    "# We will therefore create one scenario, and then split the data into train, dev and test sets by \n",
    "# \"copying\" all non-period related information, and then splitting the period related information\n",
    "if sample_data_params['split_by_period']:\n",
    "    \n",
    "    scenario = Scenario(\n",
    "        periods=None,  # period info for each dataset is given in sample_data_params\n",
    "        problem_params=problem_params, \n",
    "        store_params=store_params, \n",
    "        warehouse_params=warehouse_params, \n",
    "        echelon_params=echelon_params, \n",
    "        num_samples=params_by_dataset['train']['n_samples'],  # in this case, num_samples=number of products, which has to be the same across all datasets\n",
    "        observation_params=observation_params, \n",
    "        seeds=seeds\n",
    "        )\n",
    "    \n",
    "    train_dataset, dev_dataset, test_dataset = dataset_creator.create_datasets(\n",
    "        scenario, \n",
    "        split=True, \n",
    "        by_period=True, \n",
    "        periods_for_split=[sample_data_params[k] for  k in ['train_periods', 'dev_periods', 'test_periods']],)\n",
    "\n",
    "# For synthetic data, we will first create a scenario that we will divide into train and dev sets by sample index.\n",
    "# Then, we will create a separate scenario for the test set, which will be exaclty the same as the previous scenario, \n",
    "# but with different seeds to generate demand traces, and with a longer time horizon.\n",
    "# One can use this method of generating scenarios to train a model using some specific problem primitives, \n",
    "# and then test it on a different set of problem primitives, by simply creating a new scenario with the desired primitives.\n",
    "else:\n",
    "    max_periods = max(params_by_dataset['train']['periods'], params_by_dataset['dev']['periods'])\n",
    "    scenario = Scenario(\n",
    "        periods=max_periods, \n",
    "        # periods=params_by_dataset['train']['periods'], \n",
    "        problem_params=problem_params, \n",
    "        store_params=store_params, \n",
    "        warehouse_params=warehouse_params, \n",
    "        echelon_params=echelon_params, \n",
    "        num_samples=params_by_dataset['train']['n_samples'] + params_by_dataset['dev']['n_samples'], \n",
    "        observation_params=observation_params, \n",
    "        seeds=seeds\n",
    "        )\n",
    "\n",
    "    train_dataset, dev_dataset = dataset_creator.create_datasets(scenario, split=True, by_sample_indexes=True, sample_index_for_split=params_by_dataset['dev']['n_samples'])\n",
    "\n",
    "    scenario = Scenario(\n",
    "        params_by_dataset['test']['periods'], \n",
    "        problem_params, \n",
    "        store_params, \n",
    "        warehouse_params, \n",
    "        echelon_params, \n",
    "        params_by_dataset['test']['n_samples'], \n",
    "        observation_params, \n",
    "        test_seeds\n",
    "        )\n",
    "\n",
    "    test_dataset = dataset_creator.create_datasets(scenario, split=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=params_by_dataset['train']['batch_size'], shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=params_by_dataset['dev']['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params_by_dataset['test']['batch_size'], shuffle=False)\n",
    "data_loaders = {'train': train_loader, 'dev': dev_loader, 'test': test_loader}\n",
    "\n",
    "neural_net_creator = NeuralNetworkCreator\n",
    "model = neural_net_creator().create_neural_network(scenario, nn_params, device=device)\n",
    "\n",
    "loss_function = PolicyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_params['learning_rate'])\n",
    "\n",
    "simulator = Simulator(device=device)\n",
    "trainer = Trainer(device=device)\n",
    "\n",
    "# We will create a folder for each day of the year, and a subfolder for each model\n",
    "# When executing with different problem primitives (i.e. instance), it might be useful to create an additional subfolder for each instance\n",
    "trainer_params['base_dir'] = 'saved_models'\n",
    "trainer_params['save_model_folders'] = [trainer.get_year_month_day(), nn_params['name']]\n",
    "\n",
    "# We will simply name the model with the current time stamp\n",
    "trainer_params['save_model_filename'] = trainer.get_time_stamp()\n",
    "\n",
    "# Load previous model if load_model is set to True in the config file\n",
    "if trainer_params['load_previous_model']:\n",
    "    print(f'Loading model from {trainer_params[\"load_model_path\"]}')\n",
    "    model, optimizer = trainer.load_model(model, optimizer, trainer_params['load_model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load a model to continue training\n",
    "load_model = True\n",
    "if load_model:\n",
    "    model_filename = \"saved_models/2025_10_06/model_32_demands.pt\"\n",
    "    model, optimizer = trainer.load_model(model, optimizer, model_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training. You can stop training whenever you want. This will keep the model (ie the neural network) with the weights of the last iteration. If you want to use the model that achieved the smallest dev loss, just run one of the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Average per-period train loss: 1.3535701919955625\n",
      "Average per-period dev loss: 1.6669327468641428\n",
      "Best per-period dev loss: 1.6669327468641428\n",
      "epoch: 2\n",
      "Average per-period train loss: 1.3291083903416305\n",
      "Average per-period dev loss: 1.7054354738189776\n",
      "Best per-period dev loss: 1.6669327468641428\n",
      "epoch: 3\n",
      "Average per-period train loss: 1.3004066523131501\n",
      "Average per-period dev loss: 1.671083280693047\n",
      "Best per-period dev loss: 1.6669327468641428\n",
      "epoch: 4\n",
      "Average per-period train loss: 1.2869189060872466\n",
      "Average per-period dev loss: 1.6585238136195462\n",
      "Best per-period dev loss: 1.6585238136195462\n",
      "epoch: 5\n",
      "Average per-period train loss: 1.2774912313792217\n",
      "Average per-period dev loss: 1.6622314287889726\n",
      "Best per-period dev loss: 1.6585238136195462\n",
      "epoch: 6\n",
      "Average per-period train loss: 1.2732966374468488\n",
      "Average per-period dev loss: 1.63119715996255\n",
      "Best per-period dev loss: 1.63119715996255\n",
      "epoch: 7\n",
      "Average per-period train loss: 1.2739436799491317\n",
      "Average per-period dev loss: 1.6426088807923116\n",
      "Best per-period dev loss: 1.63119715996255\n",
      "epoch: 8\n",
      "Average per-period train loss: 1.265834252444423\n",
      "Average per-period dev loss: 1.608417781042729\n",
      "Best per-period dev loss: 1.608417781042729\n",
      "epoch: 9\n",
      "Average per-period train loss: 1.2647170224724613\n",
      "Average per-period dev loss: 1.6226072169832604\n",
      "Best per-period dev loss: 1.608417781042729\n",
      "epoch: 10\n",
      "Average per-period train loss: 1.2575277372008904\n",
      "Average per-period dev loss: 1.6136383344425393\n",
      "Best per-period dev loss: 1.608417781042729\n",
      "epoch: 11\n",
      "Average per-period train loss: 1.2587470085693035\n",
      "Average per-period dev loss: 1.637484207914091\n",
      "Best per-period dev loss: 1.608417781042729\n",
      "epoch: 12\n",
      "Average per-period train loss: 1.2721897467779275\n",
      "Average per-period dev loss: 1.6283611057844154\n",
      "Best per-period dev loss: 1.608417781042729\n",
      "epoch: 13\n",
      "Average per-period train loss: 1.2667323783718947\n",
      "Average per-period dev loss: 1.6286788862631412\n",
      "Best per-period dev loss: 1.608417781042729\n",
      "epoch: 14\n",
      "Average per-period train loss: 1.2526967187566884\n",
      "Average per-period dev loss: 1.6023321879371024\n",
      "Best per-period dev loss: 1.6023321879371024\n",
      "epoch: 15\n",
      "Average per-period train loss: 1.2455264817698302\n",
      "Average per-period dev loss: 1.586661538938772\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 16\n",
      "Average per-period train loss: 1.2415036846935064\n",
      "Average per-period dev loss: 1.6025426315819158\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 17\n",
      "Average per-period train loss: 1.2347071535937746\n",
      "Average per-period dev loss: 1.5944535938275504\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 18\n",
      "Average per-period train loss: 1.233508463290848\n",
      "Average per-period dev loss: 1.608671934925326\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 19\n",
      "Average per-period train loss: 1.2323218680216672\n",
      "Average per-period dev loss: 1.5933885092834905\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 20\n",
      "Average per-period train loss: 1.2341881261548806\n",
      "Average per-period dev loss: 1.6154202316360602\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 21\n",
      "Average per-period train loss: 1.2349549345731132\n",
      "Average per-period dev loss: 1.599763682714434\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 22\n",
      "Average per-period train loss: 1.2264566963234664\n",
      "Average per-period dev loss: 1.5996581083901096\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 23\n",
      "Average per-period train loss: 1.2298404074231626\n",
      "Average per-period dev loss: 1.6158000524522853\n",
      "Best per-period dev loss: 1.586661538938772\n",
      "epoch: 24\n",
      "Average per-period train loss: 1.2311659004992295\n",
      "Average per-period dev loss: 1.5707148809389524\n",
      "Best per-period dev loss: 1.5707148809389524\n",
      "epoch: 25\n",
      "Average per-period train loss: 1.2430565030927614\n",
      "Average per-period dev loss: 1.5986159364706944\n",
      "Best per-period dev loss: 1.5707148809389524\n",
      "epoch: 26\n",
      "Average per-period train loss: 1.2395215762025027\n",
      "Average per-period dev loss: 1.5702194394937508\n",
      "Best per-period dev loss: 1.5702194394937508\n",
      "epoch: 27\n",
      "Average per-period train loss: 1.2360971210029537\n",
      "Average per-period dev loss: 1.5828895921693813\n",
      "Best per-period dev loss: 1.5702194394937508\n",
      "epoch: 28\n",
      "Average per-period train loss: 1.231104143368399\n",
      "Average per-period dev loss: 1.580588988404097\n",
      "Best per-period dev loss: 1.5702194394937508\n",
      "epoch: 29\n",
      "Average per-period train loss: 1.224608385659682\n",
      "Average per-period dev loss: 1.5921261999165275\n",
      "Best per-period dev loss: 1.5702194394937508\n",
      "epoch: 30\n",
      "Average per-period train loss: 1.2221743994620664\n",
      "Average per-period dev loss: 1.5791382667621712\n",
      "Best per-period dev loss: 1.5702194394937508\n",
      "epoch: 31\n",
      "Average per-period train loss: 1.2206516514347274\n",
      "Average per-period dev loss: 1.57993210108063\n",
      "Best per-period dev loss: 1.5702194394937508\n",
      "epoch: 32\n",
      "Average per-period train loss: 1.2184091652932951\n",
      "Average per-period dev loss: 1.602346993017642\n",
      "Best per-period dev loss: 1.5702194394937508\n",
      "epoch: 33\n",
      "Average per-period train loss: 1.2195216993823395\n",
      "Average per-period dev loss: 1.570071741190272\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 34\n",
      "Average per-period train loss: 1.2138778612279368\n",
      "Average per-period dev loss: 1.5828199730406534\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 35\n",
      "Average per-period train loss: 1.2147076669359902\n",
      "Average per-period dev loss: 1.5838124421896855\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 36\n",
      "Average per-period train loss: 1.2156965195509253\n",
      "Average per-period dev loss: 1.5797972690971438\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 37\n",
      "Average per-period train loss: 1.213981407536421\n",
      "Average per-period dev loss: 1.5704286493818527\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 38\n",
      "Average per-period train loss: 1.2145031661119672\n",
      "Average per-period dev loss: 1.6017886299801471\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 39\n",
      "Average per-period train loss: 1.2111469335912417\n",
      "Average per-period dev loss: 1.607472899652574\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 40\n",
      "Average per-period train loss: 1.2071272020764665\n",
      "Average per-period dev loss: 1.5937025884920815\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 41\n",
      "Average per-period train loss: 1.205837812954818\n",
      "Average per-period dev loss: 1.584811080122276\n",
      "Best per-period dev loss: 1.570071741190272\n",
      "epoch: 42\n",
      "Average per-period train loss: 1.2055338067898063\n",
      "Average per-period dev loss: 1.5584744281008889\n",
      "Best per-period dev loss: 1.5584744281008889\n",
      "epoch: 43\n",
      "Average per-period train loss: 1.2073006571226261\n",
      "Average per-period dev loss: 1.6252791815187475\n",
      "Best per-period dev loss: 1.5584744281008889\n",
      "epoch: 44\n",
      "Average per-period train loss: 1.2115372631825836\n",
      "Average per-period dev loss: 1.5891286998601273\n",
      "Best per-period dev loss: 1.5584744281008889\n",
      "epoch: 45\n",
      "Average per-period train loss: 1.2069532593273333\n",
      "Average per-period dev loss: 1.5682649926115597\n",
      "Best per-period dev loss: 1.5584744281008889\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Neural_inventory_control/trainer.py:66\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, loss_function, simulator, model, data_loaders, optimizer, problem_params, observation_params, params_by_dataset, trainer_params)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mTrain a parameterized policy\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    and the metric to use for choosing the best model\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs): \u001b[38;5;66;03m# Make multiple passes through the dataset\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Do one epoch of training, including updating the model parameters\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     average_train_loss, average_train_loss_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperiods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_periods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_by_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore_periods\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_train_losses\u001b[38;5;241m.\u001b[39mappend(average_train_loss_to_report)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m trainer_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdo_dev_every_n_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Neural_inventory_control/trainer.py:163\u001b[0m, in \u001b[0;36mTrainer.do_one_epoch\u001b[0;34m(self, optimizer, data_loader, loss_function, simulator, model, periods, problem_params, observation_params, train, ignore_periods, discrete_allocation)\u001b[0m\n\u001b[1;32m    160\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m total_reward, reward_to_report \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_periods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_allocation\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_reward\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Rewards from period 0\u001b[39;00m\n\u001b[1;32m    167\u001b[0m epoch_loss_to_report \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward_to_report\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Rewards from period ignore_periods onwards\u001b[39;00m\n",
      "File \u001b[0;32m~/Neural_inventory_control/trainer.py:204\u001b[0m, in \u001b[0;36mTrainer.simulate_batch\u001b[0;34m(self, loss_function, simulator, model, periods, problem_params, data_batch, observation_params, ignore_periods, discrete_allocation)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discrete_allocation:  \u001b[38;5;66;03m# Round actions to the nearest integer if specified\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     action \u001b[38;5;241m=\u001b[39m {key: val\u001b[38;5;241m.\u001b[39mround() \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 204\u001b[0m observation, reward, terminated, _, _  \u001b[38;5;241m=\u001b[39m \u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m loss_function(\u001b[38;5;28;01mNone\u001b[39;00m, action, reward)\n\u001b[1;32m    208\u001b[0m batch_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_reward\n",
      "File \u001b[0;32m~/Neural_inventory_control/environment.py:152\u001b[0m, in \u001b[0;36mSimulator.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_time_features(\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_data, \n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation, \n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_params, \n\u001b[1;32m    148\u001b[0m     current_period\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent_period\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    149\u001b[0m     )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# Calculate reward and update store inventories\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_store_reward_and_update_store_inventories\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_demands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize_profit\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Calculate reward and update warehouse inventories\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_warehouses\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Neural_inventory_control/environment.py:242\u001b[0m, in \u001b[0;36mSimulator.calculate_store_reward_and_update_store_inventories\u001b[0;34m(self, current_demands, action, observation, maximize_profit)\u001b[0m\n\u001b[1;32m    227\u001b[0m store_lead_times \u001b[38;5;241m=\u001b[39m observation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlead_times\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# observation['store_inventories'] = self.update_inventory_for_heterogeneous_lead_times(\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m#     store_inventory, \u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m#     post_inventory_on_hand, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# print(f'lead time: {int(observation[\"lead_times\"][0, 0])}')\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# print(f'action: {action[\"stores\"][0]}')\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m observation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore_inventories\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_left_add_first_col_and_append\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_inventory_on_hand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore_inventories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlead_times\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Neural_inventory_control/environment.py:633\u001b[0m, in \u001b[0;36mSimulator.move_left_add_first_col_and_append\u001b[0;34m(self, post_inventory_on_hand, inventory, lead_time, action)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03mMove columns of inventory (deleting first column, as post_inventory_on_hand accounts for inventory_on_hand after demand arrives)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m  to the left, add inventory_on_hand to first column, and append action at the end\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# inventory_on_hand + inventory[:, :, 1],\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m    632\u001b[0m     post_inventory_on_hand \u001b[38;5;241m+\u001b[39m inventory[:, :, \u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_columns_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43minventory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \n\u001b[1;32m    634\u001b[0m     action\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Remove the last dimension to make it 2D\u001b[39;00m\n\u001b[1;32m    635\u001b[0m     ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Neural_inventory_control/environment.py:606\u001b[0m, in \u001b[0;36mSimulator.move_columns_left\u001b[0;34m(self, tensor_to_displace, start_index, end_index)\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m time_product_feature_keys:\n\u001b[1;32m    597\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_past_time_product_features(\n\u001b[1;32m    598\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_data,\n\u001b[1;32m    599\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m                 feature_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[1;32m    604\u001b[0m             )\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmove_columns_left\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_to_displace, start_index, end_index):\n\u001b[1;32m    607\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m    Move all columns in given array to the left, and return as list\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [tensor_to_displace[:, :, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_index, end_index)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    trainer_params['epochs'], \n",
    "    loss_function, simulator, \n",
    "    model, \n",
    "    data_loaders, \n",
    "    optimizer, \n",
    "    problem_params, \n",
    "    observation_params, \n",
    "    params_by_dataset, \n",
    "    trainer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set weights to the version that achieved the smallest dev loss (i.e., we are using early stopping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_state = get_best_model_state(model, trainer, optimizer, config_setting, config_hyperparams)\n",
    "model.load_state_dict(best_model_state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save the model that achieved the smallest dev_loss during training\n",
    "save_model = False\n",
    "if save_model:\n",
    "    # The function will save the model that achieved the smallest dev_loss during training\n",
    "    model_filename = \"model_32_demands\"\n",
    "    saved_model_path = save_trained_model(model_filename, model, trainer, optimizer, config_setting, config_hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load a model to evaluate. Otherwise, will use the model from training.\n",
    "load_model_for_test = False\n",
    "if load_model_for_test:\n",
    "    model_filename = \"saved_models/2025_10_06/model_32_demands.pt\"\n",
    "    model, optimizer = trainer.load_model(model, optimizer, model_filename)\n",
    "    print(f\"Loaded model from {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average per-period per-product test loss: 1.5584744281008889\n"
     ]
    }
   ],
   "source": [
    "average_test_loss, average_test_loss_to_report = trainer.test(\n",
    "    loss_function, \n",
    "    simulator, \n",
    "    model, \n",
    "    data_loaders, \n",
    "    optimizer, \n",
    "    problem_params, \n",
    "    observation_params, \n",
    "    params_by_dataset, \n",
    "    discrete_allocation=False\n",
    "    )\n",
    "\n",
    "print(f'Average per-period per-product test loss: {average_test_loss_to_report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions using the model. For this, we need `config_setting_file` and `config_hyperparams_file ` to match the ones we used during training, since this info will be used to create the state input for the neural network. If it does not exactly match, it might raise an error or (worse) we will get a prediction using wrong inputs. `inventory_state_path` is a filename to the tensor of size [products, 2] defining the inventory state for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration files...\n",
      "Loading data files...\n",
      "Making prediction...\n",
      "Past demands[0]: tensor([[[ 0.,  0.,  2.,  2.,  0.,  0.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
      "           2.,  2.]],\n",
      "\n",
      "        [[ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  3.,\n",
      "           1.,  1.]],\n",
      "\n",
      "        [[ 7., 10.,  5., 11.,  7., 13.,  8., 17.,  6., 11.,  8., 12.,  6.,  7.,\n",
      "           9.,  7.]],\n",
      "\n",
      "        [[ 4.,  8.,  6.,  7.,  9.,  7.,  6.,  8.,  8., 18., 11., 14.,  3., 12.,\n",
      "          13.,  2.]],\n",
      "\n",
      "        [[ 0.,  0.,  0.,  0.,  0.,  0.,  2.,  0.,  2.,  5.,  2.,  2.,  4.,  2.,\n",
      "           3.,  2.]],\n",
      "\n",
      "        [[27.,  0.,  0.,  0.,  2.,  0.,  0.,  2.,  3.,  0.,  0.,  0.,  0.,  2.,\n",
      "           0.,  0.]],\n",
      "\n",
      "        [[17.,  6., 11., 15., 12., 11.,  9., 13., 13., 15.,  5., 13.,  8.,  6.,\n",
      "           7., 12.]],\n",
      "\n",
      "        [[ 3.,  2.,  0.,  0.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  2.,  2.,  0.,\n",
      "           0.,  0.]],\n",
      "\n",
      "        [[16.,  0., 15.,  0.,  0.,  0.,  2.,  0., 10.,  2.,  0.,  0., 17.,  0.,\n",
      "           0.,  0.]],\n",
      "\n",
      "        [[ 1.,  4.,  2.,  0.,  4.,  7., 12.,  8.,  4.,  7., 10.,  9.,  6.,  6.,\n",
      "          10.,  7.]]], device='cuda:0')\n",
      "Past stockouts[0]: tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       device='cuda:0')\n",
      "Inventory state[0]: tensor([[[ 3.,  3.]],\n",
      "\n",
      "        [[ 1.,  1.]],\n",
      "\n",
      "        [[ 6.,  6.]],\n",
      "\n",
      "        [[ 9.,  7.]],\n",
      "\n",
      "        [[ 3.,  1.]],\n",
      "\n",
      "        [[ 4.,  2.]],\n",
      "\n",
      "        [[11., 10.]],\n",
      "\n",
      "        [[ 2.,  1.]],\n",
      "\n",
      "        [[ 4.,  4.]],\n",
      "\n",
      "        [[ 7.,  7.]]], device='cuda:0')\n",
      "Time features[0]: tensor([105.,   8.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.], device='cuda:0')\n",
      "Sample predictions (first 10): tensor([[[ 1.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[ 9.]],\n",
      "\n",
      "        [[ 6.]],\n",
      "\n",
      "        [[ 3.]],\n",
      "\n",
      "        [[ 2.]],\n",
      "\n",
      "        [[11.]],\n",
      "\n",
      "        [[ 1.]],\n",
      "\n",
      "        [[ 4.]],\n",
      "\n",
      "        [[ 7.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Use with different config files\n",
    "predictions = make_predictions_from_config(\n",
    "    model, device,\n",
    "    config_setting_file=config_setting_file,\n",
    "    config_hyperparams_file=config_hyperparams_file,\n",
    "    inventory_state_path='vn2_processed_data/all_data/inventory_state.pt',\n",
    "    data_dir='vn2_processed_data/all_data/',\n",
    "    round_predictions=True # we round the outputs to the nearest integer\n",
    ")\n",
    "\n",
    "# optional: save predictions in submission format\n",
    "save_the_predictions = False\n",
    "if save_the_predictions:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"predictions/submission_{timestamp}.csv\"\n",
    "    save_predictions_to_submission_format(\n",
    "        predictions,\n",
    "        output_filename=output_filename,\n",
    "        reference_data_path=\"vn2_data/Week 0 - 2024-04-08 - Initial State.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_inventory_control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
